"""
OpenCV å›¾åƒå¤„ç†æœåŠ¡æ¨¡å—
"""
import os
import cv2
import numpy as np
from PIL import Image
import tempfile
import base64
from flask import current_app
from ..utils.helpers import save_uploaded_file, allowed_file


class OpenCVService:
    def __init__(self):
        # åˆå§‹åŒ– YOLO æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.yolo_net = None
        self.yolo_classes = None
        self.yolo_output_layers = None
        self._load_yolo_model()

        # åˆå§‹åŒ– Haar Cascade åˆ†ç±»å™¨
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

    def _load_yolo_model(self):
        """åŠ è½½ YOLO æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½é¢„è®­ç»ƒçš„ YOLO æ¨¡å‹
            # ç”±äºæ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            pass
        except Exception as e:
            print(f"YOLO æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    def detect_objects_opencv(self, file=None, image_data=None, method='contour', object_name='å¯¹è±¡'):
        """ä½¿ç”¨ OpenCV è¿›è¡Œç›®æ ‡æ£€æµ‹"""
        try:
            # å¤„ç†æ–‡ä»¶è¾“å…¥
            if image_data:
                image_data_clean = image_data.split(',')[1] if ',' in image_data else image_data
                image_bytes = base64.b64decode(image_data_clean)
                with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                    tmp_file.write(image_bytes)
                    filepath = tmp_file.name
            else:
                if not file or file.filename == '':
                    return {'success': False, 'error': 'æœªé€‰æ‹©æ–‡ä»¶'}, 400
                if not allowed_file(file.filename):
                    return {'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶ç±»å‹'}, 400
                # æ£€æŸ¥æ˜¯å¦æ˜¯å·²ç»å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæµ‹è¯•ï¼‰
                if hasattr(file, 'filepath') and os.path.exists(file.filepath):
                    filepath = file.filepath
                else:
                    # æ ‡å‡†çš„Flaskæ–‡ä»¶ä¸Šä¼ å¯¹è±¡
                    filepath = save_uploaded_file(file, current_app.config['UPLOAD_FOLDER'])

            # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæœ‰æ•ˆ
            if not os.path.exists(filepath):
                error_msg = f'å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {filepath}'
                print(error_msg)
                return {'success': False, 'error': error_msg}, 400

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath)
            if file_size == 0:
                error_msg = f'å›¾åƒæ–‡ä»¶ä¸ºç©º: {filepath}ï¼ˆæ–‡ä»¶å¤§å°: 0 å­—èŠ‚ï¼‰'
                print(error_msg)
                return {'success': False, 'error': error_msg}, 400

            if file_size < 100:  # å°äº100å­—èŠ‚çš„å›¾åƒæ–‡ä»¶é€šå¸¸æ˜¯æ— æ•ˆçš„
                error_msg = f'å›¾åƒæ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½å·²æŸå: {filepath}ï¼ˆæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ï¼‰'
                print(error_msg)
                return {'success': False, 'error': error_msg}, 400

            # è¯»å–å›¾åƒ - ä½¿ç”¨å¤šç§æ–¹æ³•ç¡®ä¿èƒ½å¤Ÿè¯»å–
            image = None

            # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨cv2.imread
            try:
                image = cv2.imread(filepath)
                if image is not None and image.size > 0:
                    print("ä½¿ç”¨cv2.imreadæˆåŠŸè¯»å–å›¾åƒ")
                else:
                    image = None
            except Exception as e:
                print(f"cv2.imread å¤±è´¥: {e}")

            # æ–¹æ³•2: å¦‚æœç›´æ¥è¯»å–å¤±è´¥ï¼Œä½¿ç”¨PILè½¬æ¢
            if image is None:
                try:
                    from PIL import Image as PILImage
                    pil_image = PILImage.open(filepath)

                    # æ£€æŸ¥å›¾åƒæ˜¯å¦æœ‰æ•ˆ
                    if pil_image.size[0] == 0 or pil_image.size[1] == 0:
                        raise ValueError("å›¾åƒå°ºå¯¸æ— æ•ˆ")

                    # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœæ˜¯RGBAï¼‰
                    if pil_image.mode == 'RGBA':
                        pil_image = pil_image.convert('RGB')
                    elif pil_image.mode == 'P':
                        pil_image = pil_image.convert('RGB')

                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    image_array = np.array(pil_image)
                    # PILä½¿ç”¨RGBï¼ŒOpenCVä½¿ç”¨BGRï¼Œéœ€è¦è½¬æ¢
                    image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
                    print("ä½¿ç”¨PILæˆåŠŸè¯»å–å›¾åƒ")
                except Exception as e:
                    print(f"PILè¯»å–å¤±è´¥: {e}")

            # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨numpyç›´æ¥è¯»å–
            if image is None:
                try:
                    # è¯»å–æ–‡ä»¶å­—èŠ‚
                    with open(filepath, 'rb') as f:
                        file_bytes = f.read()

                    if len(file_bytes) == 0:
                        raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")

                    # ä½¿ç”¨numpyå’Œcv2è§£ç 
                    nparr = np.frombuffer(file_bytes, np.uint8)
                    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                    if image is not None and image.size > 0:
                        print("ä½¿ç”¨numpyæˆåŠŸè¯»å–å›¾åƒ")
                    else:
                        image = None
                except Exception as e:
                    print(f"numpyè¯»å–å¤±è´¥: {e}")

            if image is None:
                error_msg = f'æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {filepath}ã€‚æ–‡ä»¶å¯èƒ½å·²æŸåæˆ–æ ¼å¼ä¸æ”¯æŒã€‚æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚'
                print(error_msg)
                return {'success': False, 'error': error_msg}, 400

            # éªŒè¯å›¾åƒå°ºå¯¸
            if image.shape[0] == 0 or image.shape[1] == 0:
                error_msg = f'å›¾åƒå°ºå¯¸æ— æ•ˆ: {filepath}ã€‚å›¾åƒå°ºå¯¸: {image.shape}'
                print(error_msg)
                return {'success': False, 'error': error_msg}, 400

            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†ç‰¹å®šå¯¹è±¡åç§°ï¼Œè¿›è¡Œå†…å®¹éªŒè¯
            if object_name and object_name.strip() and object_name.strip() != 'å¯¹è±¡':
                validation_result = self._validate_image_content(filepath, object_name.strip())
                if not validation_result['is_match']:
                    return {
                        'success': False,
                        'error': f'æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼š{object_name.strip()}',
                        'message': validation_result['message'],
                        'suggestion': validation_result.get('suggestion', 'è¯·æ£€æŸ¥å›¾åƒå†…å®¹æˆ–ä¿®æ”¹æŸ¥è¯¢è¯æ±‡ã€‚'),
                        'detected_objects': validation_result.get('detected_objects', []),
                        'alternative_queries': validation_result.get('alternative_queries', [])
                    }, 200  # æ”¹ä¸º200çŠ¶æ€ç ï¼Œè®©å‰ç«¯æ­£ç¡®å¤„ç†å†…å®¹ä¸åŒ¹é…

            detected_objects = []

            if method == 'haar':
                # ä½¿ç”¨ Haar Cascade æ£€æµ‹ï¼ˆä»…é™äººè„¸ï¼‰
                # éªŒè¯æ˜¯å¦ä¸ç”¨æˆ·æŸ¥è¯¢åŒ¹é…
                if object_name and object_name.strip() and 'äººè„¸' not in object_name and 'è„¸' not in object_name and 'face' not in object_name.lower():
                    return {
                        'success': False,
                        'error': f'æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼š{object_name}',
                        'message': f'Haar Cascadeæ–¹æ³•ä»…æ”¯æŒäººè„¸æ£€æµ‹ï¼Œä½†æ‚¨æŸ¥è¯¢çš„æ˜¯"{object_name}"',
                        'suggestion': 'è¯·ä½¿ç”¨å…¶ä»–æ£€æµ‹æ–¹æ³•æˆ–ä¿®æ”¹æŸ¥è¯¢ä¸º"äººè„¸"'
                    }, 200  # æ”¹ä¸º200çŠ¶æ€ç 
                detected_objects = self._detect_faces_haar(image)
            elif method == 'contour':
                # ä½¿ç”¨è½®å»“æ£€æµ‹ï¼ˆé€šç”¨å¯¹è±¡æ£€æµ‹ï¼‰
                # é¦–å…ˆéªŒè¯å›¾åƒå†…å®¹æ˜¯å¦åŒ…å«ç”¨æˆ·æŸ¥è¯¢çš„å¯¹è±¡
                if object_name and object_name.strip():
                    content_match = self._validate_image_content(filepath, object_name)
                    if not content_match['is_match']:
                        return {
                            'success': False,
                            'error': f'æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼š{object_name}',
                            'message': content_match["message"],
                            'suggestion': content_match.get('suggestion', 'è¯·æ£€æŸ¥å›¾åƒå†…å®¹æˆ–ä¿®æ”¹æŸ¥è¯¢è¯æ±‡ã€‚'),
                            'detected_objects': content_match.get('detected_objects', []),
                            'alternative_queries': content_match.get('alternative_queries', [])
                        }, 200  # æ”¹ä¸º200çŠ¶æ€ç 
                detected_objects = self._detect_contours(image, object_name)
            elif method == 'color':
                # ä½¿ç”¨é¢œè‰²åˆ†å‰²æ£€æµ‹
                # éªŒè¯å›¾åƒå†…å®¹
                if object_name and object_name.strip():
                    content_match = self._validate_image_content(filepath, object_name)
                    if not content_match['is_match']:
                        return {
                            'success': False,
                            'error': f'æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼š{object_name}',
                            'message': content_match["message"],
                            'suggestion': content_match.get('suggestion', 'è¯·æ£€æŸ¥å›¾åƒå†…å®¹æˆ–ä¿®æ”¹æŸ¥è¯¢è¯æ±‡ã€‚'),
                            'detected_objects': content_match.get('detected_objects', []),
                            'alternative_queries': content_match.get('alternative_queries', [])
                        }, 200  # æ”¹ä¸º200çŠ¶æ€ç 
                detected_objects = self._detect_by_color(image, object_name)
            elif method == 'edge':
                # ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹
                # éªŒè¯å›¾åƒå†…å®¹
                if object_name and object_name.strip():
                    content_match = self._validate_image_content(filepath, object_name)
                    if not content_match['is_match']:
                        return {
                            'success': False,
                            'error': f'æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼š{object_name}',
                            'message': content_match["message"],
                            'suggestion': content_match.get('suggestion', 'è¯·æ£€æŸ¥å›¾åƒå†…å®¹æˆ–ä¿®æ”¹æŸ¥è¯¢è¯æ±‡ã€‚'),
                            'detected_objects': content_match.get('detected_objects', []),
                            'alternative_queries': content_match.get('alternative_queries', [])
                        }, 200  # æ”¹ä¸º200çŠ¶æ€ç 
                detected_objects = self._detect_by_edges(image, object_name)

            # ç”Ÿæˆå¸¦è¾¹ç•Œæ¡†çš„å›¾åƒ
            bbox_images = []
            for i, obj in enumerate(detected_objects):
                bbox_filename = f"opencv_bbox_{obj['label']}_{i}_{os.path.basename(filepath)}"
                bbox_filepath = os.path.join(current_app.config['GENERATED_FOLDER'], bbox_filename)
                self._draw_opencv_bbox(filepath, obj['bbox'], bbox_filepath, obj['label'], obj['method'], i)
                bbox_images.append(bbox_filepath)

            if detected_objects:
                # åˆ›å»ºæ±‡æ€»å›¾ç‰‡
                summary_filename = f"opencv_summary_{os.path.basename(filepath)}"
                summary_filepath = os.path.join(current_app.config['GENERATED_FOLDER'], summary_filename)
                self._draw_all_opencv_bboxes(filepath, detected_objects, summary_filepath)

                return {
                    'success': True,
                    'detected_objects': detected_objects,
                    'original_image': filepath,
                    'bbox_images': bbox_images,
                    'summary_image': summary_filepath,
                    'method': f'OpenCV {method}'
                }, 200
            else:
                return {
                    'success': False,
                    'error': f'ä½¿ç”¨ OpenCV {method} æ–¹æ³•æœªæ£€æµ‹åˆ°å¯¹è±¡',
                    'method': f'OpenCV {method}'
                }, 200

        except Exception as e:
            error_msg = f'OpenCV æ£€æµ‹å¤±è´¥: {str(e)}'
            print(f"OpenCV æ£€æµ‹é”™è¯¯: {e}")
            return {'success': False, 'error': error_msg}, 500

    def _validate_image_content(self, image_path, object_name):
        """æ”¹è¿›çš„å›¾åƒå†…å®¹éªŒè¯æ–¹æ³• - æ›´ä¸¥æ ¼çš„éªŒè¯ç­–ç•¥"""
        try:
            print(f"OpenCVå†…å®¹éªŒè¯ï¼šå¼€å§‹éªŒè¯å›¾åƒæ˜¯å¦åŒ…å« '{object_name}'")

            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯é€šç”¨æŸ¥è¯¢è¯ï¼Œå¦‚æœæ˜¯åˆ™è¦æ±‚ç”¨æˆ·è¾“å…¥å…·ä½“å¯¹è±¡
            generic_queries = ['å¯¹è±¡', 'ç‰©ä½“', 'ä¸œè¥¿', 'ä¸»è¦å¯¹è±¡', 'object', 'thing', 'item']
            if object_name.strip().lower() in [q.lower() for q in generic_queries]:
                return {
                    'is_match': False,
                    'message': f'è¯·è¾“å…¥å…·ä½“çš„å¯¹è±¡åç§°ï¼Œè€Œä¸æ˜¯é€šç”¨è¯æ±‡"{object_name}"',
                    'suggestion': 'è¯·æŒ‡å®šæ‚¨æƒ³è¦åˆ†å‰²çš„å…·ä½“å¯¹è±¡ï¼Œå¦‚ï¼šäººã€ç‹—ã€æ±½è½¦ã€èŠ±æœµç­‰',
                    'detected_objects': [],
                    'validation_method': 'é€šç”¨è¯æ±‡æ‹’ç»',
                    'is_generic_query': True
                }

            # æ–¹æ³•1: é¦–å…ˆå°è¯•ä½¿ç”¨YOLOè¿›è¡Œå¿«é€ŸéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            yolo_validation = self._validate_with_yolo(image_path, object_name)
            if yolo_validation['is_available']:
                if yolo_validation['is_match']:
                    print(f"YOLOéªŒè¯æˆåŠŸï¼šæ£€æµ‹åˆ° '{object_name}'")
                    return {
                        'is_match': True,
                        'message': f'YOLOæ£€æµ‹åˆ°åŒ¹é…çš„å¯¹è±¡: {object_name}',
                        'validation_method': 'YOLOå¿«é€ŸéªŒè¯',
                        'detected_objects': yolo_validation.get('detected_objects', [])
                    }
                else:
                    # YOLOæ˜ç¡®è¡¨ç¤ºä¸åŒ¹é…ï¼Œç›´æ¥è¿”å›å¤±è´¥
                    print(f"YOLOéªŒè¯å¤±è´¥ï¼šæœªæ£€æµ‹åˆ° '{object_name}'")
                    detected_objects = yolo_validation.get('detected_objects', [])
                    return {
                        'is_match': False,
                        'message': f'æœªæ£€æµ‹åˆ°"{object_name}"ã€‚å›¾åƒä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡: {", ".join(detected_objects[:5])}',
                        'suggestion': f'è¯·å°è¯•æ£€æµ‹å›¾åƒä¸­å®é™…å­˜åœ¨çš„å¯¹è±¡ï¼Œå¦‚ï¼š{", ".join(detected_objects[:3])}' if detected_objects else 'è¯·ä¸Šä¼ åŒ…å«æ˜ç¡®å¯¹è±¡çš„å›¾åƒ',
                        'detected_objects': detected_objects,
                        'validation_method': 'YOLOéªŒè¯å¤±è´¥',
                        'yolo_available': True
                    }

            # æ–¹æ³•2: å¦‚æœYOLOä¸å¯ç”¨ï¼Œä½¿ç”¨Geminiè¿›è¡Œè¯¦ç»†éªŒè¯
            gemini_validation = self._validate_with_gemini(image_path, object_name)
            if gemini_validation.get('is_available', True):
                if gemini_validation['is_match']:
                    print(f"GeminiéªŒè¯æˆåŠŸï¼šæ£€æµ‹åˆ° '{object_name}'")
                    return {
                        'is_match': True,
                        'message': f'Geminiæ£€æµ‹åˆ°åŒ¹é…çš„å¯¹è±¡: {object_name}',
                        'validation_method': 'Geminiæ™ºèƒ½éªŒè¯',
                        'detected_objects': gemini_validation.get('detected_objects', [])
                    }
                else:
                    # Geminiæ˜ç¡®è¡¨ç¤ºä¸åŒ¹é…
                    print(f"GeminiéªŒè¯å¤±è´¥ï¼šæœªæ£€æµ‹åˆ° '{object_name}'")
                    detected_objects = gemini_validation.get('detected_objects', [])
                    return {
                        'is_match': False,
                        'message': f'GeminiéªŒè¯æœªæ£€æµ‹åˆ°"{object_name}"ã€‚å›¾åƒä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡: {", ".join(detected_objects[:5])}',
                        'suggestion': f'è¯·å°è¯•æ£€æµ‹å›¾åƒä¸­å®é™…å­˜åœ¨çš„å¯¹è±¡ï¼Œå¦‚ï¼š{", ".join(detected_objects[:3])}' if detected_objects else 'è¯·ä¸Šä¼ åŒ…å«æ˜ç¡®å¯¹è±¡çš„å›¾åƒ',
                        'detected_objects': detected_objects,
                        'validation_method': 'GeminiéªŒè¯å¤±è´¥',
                        'gemini_available': True
                    }

            # æ–¹æ³•3: å¦‚æœå‰ä¸¤ç§æ–¹æ³•éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨OpenCVåŸºç¡€ç‰¹å¾éªŒè¯ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            opencv_validation = self._validate_with_opencv_features(image_path, object_name)
            if opencv_validation['is_match']:
                print(f"OpenCVç‰¹å¾éªŒè¯æˆåŠŸï¼šæ£€æµ‹åˆ°ç›¸å…³ç‰¹å¾")
                return {
                    'is_match': True,
                    'message': f'OpenCVæ£€æµ‹åˆ°ç›¸å…³ç‰¹å¾',
                    'validation_method': 'OpenCVç‰¹å¾éªŒè¯',
                    'detected_objects': opencv_validation.get('detected_objects', []),
                    'opencv_limitation': True  # æ ‡è®°è¿™æ˜¯OpenCVçš„é™åˆ¶æ€§éªŒè¯
                }

            # æ‰€æœ‰éªŒè¯æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯
            print(f"æ‰€æœ‰éªŒè¯æ–¹æ³•éƒ½å¤±è´¥ï¼Œæœªæ£€æµ‹åˆ° '{object_name}'")

            # æ”¶é›†æ‰€æœ‰æ£€æµ‹åˆ°çš„å¯¹è±¡
            all_detected = []
            if yolo_validation.get('detected_objects'):
                all_detected.extend(yolo_validation['detected_objects'])
            if gemini_validation.get('detected_objects'):
                all_detected.extend(gemini_validation['detected_objects'])
            if opencv_validation.get('detected_objects'):
                all_detected.extend(opencv_validation['detected_objects'])

            # å»é‡
            unique_detected = list(set(all_detected))

            return {
                'is_match': False,
                'message': f'å¤šé‡éªŒè¯å‡æœªæ£€æµ‹åˆ°"{object_name}"ã€‚å›¾åƒä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡: {", ".join(unique_detected[:5])}' if unique_detected else f'å¤šé‡éªŒè¯å‡æœªæ£€æµ‹åˆ°"{object_name}"ï¼Œä¸”å›¾åƒä¸­æœªè¯†åˆ«åˆ°æ˜ç¡®å¯¹è±¡',
                'suggestion': f'è¯·å°è¯•æ£€æµ‹å›¾åƒä¸­å®é™…å­˜åœ¨çš„å¯¹è±¡ï¼Œå¦‚ï¼š{", ".join(unique_detected[:3])}' if unique_detected else 'è¯·ä¸Šä¼ åŒ…å«æ˜ç¡®å¯¹è±¡çš„å›¾åƒï¼Œæˆ–æ£€æŸ¥å›¾åƒè´¨é‡',
                'detected_objects': unique_detected,
                'validation_method': 'å¤šé‡éªŒè¯å¤±è´¥',
                'yolo_available': yolo_validation.get('is_available', False),
                'gemini_available': gemini_validation.get('is_available', True)
            }

        except Exception as e:
            print(f"å†…å®¹éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
            # éªŒè¯è¿‡ç¨‹å‡ºé”™æ—¶ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œæ‹’ç»ç»§ç»­å¤„ç†
            return {
                'is_match': False,
                'message': f'å†…å®¹éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}',
                'suggestion': 'è¯·é‡è¯•æˆ–æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦æ­£ç¡®',
                'validation_method': 'éªŒè¯å¼‚å¸¸-æ‹’ç»å¤„ç†',
                'detected_objects': []
            }

    def _validate_with_yolo(self, image_path, object_name):
        """ä½¿ç”¨YOLOè¿›è¡Œå†…å®¹éªŒè¯"""
        try:
            # å°è¯•å¯¼å…¥YOLOç›¸å…³æ¨¡å—
            from ultralytics import YOLO
            import cv2

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„YOLOæ¨¡å‹
            yolo_model_path = 'yolo11n.pt'  # ä½¿ç”¨æ£€æµ‹æ¨¡å‹è€Œä¸æ˜¯åˆ†å‰²æ¨¡å‹
            if not os.path.exists(yolo_model_path):
                return {
                    'is_available': False,
                    'is_match': False,
                    'message': 'YOLOæ¨¡å‹ä¸å¯ç”¨'
                }

            # åŠ è½½YOLOæ¨¡å‹
            model = YOLO(yolo_model_path)

            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                return {
                    'is_available': True,
                    'is_match': False,
                    'message': 'æ— æ³•è¯»å–å›¾åƒ'
                }

            # è¿›è¡Œæ£€æµ‹
            results = model(image, conf=0.3)  # ä½¿ç”¨è¾ƒä½çš„ç½®ä¿¡åº¦

            detected_objects = []
            for result in results:
                if result.boxes is not None:
                    classes = result.boxes.cls.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()

                    for cls, conf in zip(classes, confidences):
                        class_name = model.names[int(cls)]
                        detected_objects.append(class_name)

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç”¨æˆ·æŸ¥è¯¢
            is_match = self._check_object_match(object_name, detected_objects)

            return {
                'is_available': True,
                'is_match': is_match,
                'detected_objects': detected_objects,
                'message': f'YOLOæ£€æµ‹åˆ°: {", ".join(detected_objects[:5])}'
            }

        except ImportError:
            return {
                'is_available': False,
                'is_match': False,
                'message': 'YOLOæ¨¡å—ä¸å¯ç”¨'
            }
        except Exception as e:
            return {
                'is_available': False,
                'is_match': False,
                'message': f'YOLOéªŒè¯å‡ºé”™: {str(e)}'
            }

    def _validate_with_gemini(self, image_path, object_name):
        """ä½¿ç”¨Geminiè¿›è¡Œæ™ºèƒ½å†…å®¹éªŒè¯ - å……åˆ†åˆ©ç”¨AIçš„è¯­ä¹‰ç†è§£èƒ½åŠ›"""
        try:
            from google import genai
            from google.genai import types
            from flask import current_app
            from ..utils.helpers import image_to_bytes

            # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
            client = genai.Client(api_key=current_app.config['GEMINI_API_KEY'])

            # è¯»å–å›¾åƒ
            image_bytes = image_to_bytes(image_path)

            # æ„å»ºæ™ºèƒ½éªŒè¯æç¤ºè¯ - åˆ©ç”¨Geminiçš„å¼ºå¤§ç†è§£èƒ½åŠ›
            validation_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒåˆ†æAIåŠ©æ‰‹ã€‚è¯·ä»”ç»†åˆ†æè¿™å¼ å›¾åƒï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«ç”¨æˆ·æŸ¥è¯¢çš„å¯¹è±¡ï¼š"{object_name.strip()}"

            åˆ†æè¦æ±‚ï¼š
            1. ğŸ” ä»”ç»†è§‚å¯Ÿå›¾åƒä¸­çš„æ‰€æœ‰å¯¹è±¡ã€äººç‰©ã€åŠ¨ç‰©ã€ç‰©å“ç­‰
            2. ğŸ§  è¿ç”¨è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œè€ƒè™‘ä»¥ä¸‹åŒ¹é…æƒ…å†µï¼š
               - ç›´æ¥åŒ¹é…ï¼šå›¾åƒä¸­ç¡®å®æœ‰è¯¥å¯¹è±¡
               - åŒä¹‰è¯åŒ¹é…ï¼šå¦‚"æ±½è½¦"ä¸"car"ã€"ç‹—"ä¸"dog"
               - ä¸Šä¸‹çº§å…³ç³»ï¼šå¦‚"å“ˆå£«å¥‡"å±äº"ç‹—"ï¼Œ"ç‹—"å±äº"åŠ¨ç‰©"
               - ç›¸å…³å¯¹è±¡ï¼šå¦‚æŸ¥è¯¢"å® ç‰©"æ—¶å›¾åƒæœ‰"ç‹—"æˆ–"çŒ«"
               - æ¨¡ç³Šæè¿°ï¼šå¦‚"æ¯›èŒ¸èŒ¸çš„åŠ¨ç‰©"å¯ä»¥åŒ¹é…å„ç§æ¯›å‘åŠ¨ç‰©
            3. ğŸ¯ åˆ¤æ–­æ ‡å‡†ï¼š
               - ä¸¥æ ¼åŒ¹é…ï¼šå›¾åƒä¸­æ˜ç¡®åŒ…å«æŸ¥è¯¢å¯¹è±¡
               - è¯­ä¹‰åŒ¹é…ï¼šå›¾åƒä¸­æœ‰è¯­ä¹‰ç›¸å…³çš„å¯¹è±¡
               - ä¸åŒ¹é…ï¼šå›¾åƒä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³å¯¹è±¡

            è¯·ä»¥JSONæ ¼å¼è¿”å›è¯¦ç»†åˆ†æç»“æœï¼š
            {{
                "contains_object": true/false,
                "match_type": "direct/semantic/none",
                "confidence": 0.95,
                "detected_objects": ["å…·ä½“æ£€æµ‹åˆ°çš„å¯¹è±¡1", "å¯¹è±¡2", ...],
                "matching_objects": ["ä¸æŸ¥è¯¢åŒ¹é…çš„å¯¹è±¡1", "å¯¹è±¡2", ...],
                "explanation": "è¯¦ç»†è¯´æ˜åŒ¹é…æˆ–ä¸åŒ¹é…çš„åŸå› ",
                "semantic_relationship": "è¯´æ˜è¯­ä¹‰å…³ç³»ï¼Œå¦‚ï¼šç‹—å±äºåŠ¨ç‰©ç±»åˆ«",
                "suggestions": ["å¦‚æœä¸åŒ¹é…ï¼Œå»ºè®®æŸ¥è¯¢çš„å¯¹è±¡1", "å¯¹è±¡2", ...]
            }}

            ç‰¹åˆ«æ³¨æ„ï¼š
            - å¦‚æœæŸ¥è¯¢å¯¹è±¡æ˜¯é€šç”¨è¯æ±‡ï¼ˆå¦‚"å¯¹è±¡"ã€"ä¸œè¥¿"ï¼‰ï¼Œè¯·åœ¨explanationä¸­æŒ‡å‡ºè¿™æ˜¯æ— æ•ˆæŸ¥è¯¢
            - å¦‚æœå›¾åƒè´¨é‡ä¸ä½³æˆ–æ— æ³•è¯†åˆ«å†…å®¹ï¼Œè¯·å¦‚å®è¯´æ˜
            - å¯¹äºè¾¹ç¼˜æƒ…å†µï¼Œè¯·ç»™å‡ºåˆç†çš„ç½®ä¿¡åº¦è¯„åˆ†
            - å……åˆ†åˆ©ç”¨ä½ çš„å¸¸è¯†å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›è¿›è¡Œåˆ¤æ–­

            ç”¨æˆ·æŸ¥è¯¢ï¼š"{object_name.strip()}"
            """

            # è¿›è¡Œæ™ºèƒ½å†…å®¹éªŒè¯
            response = client.models.generate_content(
                model=current_app.config['GEMINI_VISION_MODEL'],
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type="image/jpeg"),
                    types.Part.from_text(text=validation_prompt)
                ]
            )

            # è§£æå“åº”
            import json
            import re

            response_text = response.text.strip()
            print(f"GeminiåŸå§‹å“åº”: {response_text[:200]}...")

            # å°è¯•æå–JSON
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    validation_result = json.loads(json_match.group())

                    contains_object = validation_result.get('contains_object', False)
                    match_type = validation_result.get('match_type', 'none')
                    confidence = validation_result.get('confidence', 0.5)
                    detected_objects = validation_result.get('detected_objects', [])
                    matching_objects = validation_result.get('matching_objects', [])
                    explanation = validation_result.get('explanation', '')
                    semantic_relationship = validation_result.get('semantic_relationship', '')
                    suggestions = validation_result.get('suggestions', [])

                    # æ™ºèƒ½åˆ¤æ–­é€»è¾‘
                    is_match = False
                    if contains_object:
                        if match_type == 'direct' and confidence > 0.7:
                            is_match = True
                        elif match_type == 'semantic' and confidence > 0.6:
                            is_match = True
                        elif confidence > 0.8:  # é«˜ç½®ä¿¡åº¦æƒ…å†µ
                            is_match = True

                    # åˆå¹¶æ‰€æœ‰ç›¸å…³å¯¹è±¡
                    all_objects = list(set(detected_objects + matching_objects))

                    return {
                        'is_available': True,
                        'is_match': is_match,
                        'detected_objects': all_objects,
                        'matching_objects': matching_objects,
                        'explanation': explanation,
                        'semantic_relationship': semantic_relationship,
                        'confidence': confidence,
                        'match_type': match_type,
                        'suggestions': suggestions,
                        'gemini_analysis': {
                            'contains_object': contains_object,
                            'match_type': match_type,
                            'confidence': confidence
                        }
                    }

                except json.JSONDecodeError as e:
                    print(f"JSONè§£æå¤±è´¥: {e}")
                    # ç»§ç»­ä½¿ç”¨æ–‡æœ¬åˆ†æä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ

            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºçš„æ–‡æœ¬åˆ†æ
            return self._fallback_text_analysis(response_text, object_name)

        except Exception as e:
            print(f"GeminiéªŒè¯å¤±è´¥: {str(e)}")
            return {
                'is_available': False,
                'is_match': False,
                'message': f'GeminiéªŒè¯å‡ºé”™: {str(e)}'
            }

    def _fallback_text_analysis(self, response_text, object_name):
        """å¤‡é€‰çš„æ–‡æœ¬åˆ†ææ–¹æ³• - å½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨"""
        object_name_lower = object_name.lower()
        response_lower = response_text.lower()

        # å¢å¼ºçš„åŒ¹é…æŒ‡æ ‡
        positive_indicators = [
            f'åŒ…å«{object_name_lower}',
            f'æœ‰{object_name_lower}',
            f'å­˜åœ¨{object_name_lower}',
            f'æ£€æµ‹åˆ°{object_name_lower}',
            f'å‘ç°{object_name_lower}',
            f'çœ‹åˆ°{object_name_lower}',
            f'contains {object_name_lower}',
            f'has {object_name_lower}',
            f'detected {object_name_lower}',
            f'found {object_name_lower}',
            f'shows {object_name_lower}',
            f'includes {object_name_lower}',
            'true',
            'æ˜¯çš„',
            'ç¡®å®',
            'åŒ¹é…'
        ]

        negative_indicators = [
            f'æ²¡æœ‰{object_name_lower}',
            f'ä¸åŒ…å«{object_name_lower}',
            f'æœªæ£€æµ‹åˆ°{object_name_lower}',
            f'no {object_name_lower}',
            f'does not contain {object_name_lower}',
            f'not found {object_name_lower}',
            'false',
            'ä¸æ˜¯',
            'æ²¡æœ‰',
            'ä¸åŒ¹é…'
        ]

        # è®¡ç®—åŒ¹é…åˆ†æ•°
        positive_score = sum(1 for indicator in positive_indicators if indicator in response_lower)
        negative_score = sum(1 for indicator in negative_indicators if indicator in response_lower)

        # æå–å¯èƒ½çš„æ£€æµ‹å¯¹è±¡
        detected_objects = []
        # ç®€å•çš„å¯¹è±¡æå–é€»è¾‘
        common_objects = ['dog', 'cat', 'person', 'car', 'tree', 'house', 'bird', 'flower', 'chair', 'table']
        for obj in common_objects:
            if obj in response_lower:
                detected_objects.append(obj)

        is_match = positive_score > negative_score and positive_score > 0
        confidence = min(0.9, max(0.1, positive_score / max(1, positive_score + negative_score)))

        return {
            'is_available': True,
            'is_match': is_match,
            'detected_objects': detected_objects,
            'explanation': response_text[:300] + '...' if len(response_text) > 300 else response_text,
            'confidence': confidence,
            'match_type': 'text_analysis',
            'analysis_scores': {
                'positive': positive_score,
                'negative': negative_score
            }
        }

    def _validate_with_opencv_features(self, image_path, object_name):
        """ä½¿ç”¨OpenCVç‰¹å¾è¿›è¡ŒåŸºç¡€éªŒè¯ - æ›´ä¸¥æ ¼çš„éªŒè¯"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return {
                    'is_match': False,
                    'detected_objects': [],
                    'message': 'æ— æ³•è¯»å–å›¾åƒ'
                }

            detected_features = []

            # 1. äººè„¸æ£€æµ‹ï¼ˆå¦‚æœæŸ¥è¯¢ä¸äººç›¸å…³ï¼‰
            if any(keyword in object_name.lower() for keyword in ['äºº', 'è„¸', 'å¤´', 'person', 'face', 'head', 'ç”·', 'å¥³', 'å°å­©', 'å„¿ç«¥']):
                face_features = self._detect_face_features(image)
                if face_features:
                    detected_features.extend(face_features)
                    # å¦‚æœæ£€æµ‹åˆ°äººè„¸ï¼Œç›´æ¥è¿”å›åŒ¹é…
                    return {
                        'is_match': True,
                        'detected_objects': face_features,
                        'message': f'OpenCVæ£€æµ‹åˆ°äººè„¸ç‰¹å¾: {", ".join(face_features)}'
                    }

            # 2. é¢œè‰²ç‰¹å¾æ£€æµ‹ï¼ˆä»…å½“æŸ¥è¯¢æ˜ç¡®åŒ…å«é¢œè‰²è¯æ—¶ï¼‰
            color_features = self._detect_color_features(image, object_name)
            if color_features:
                detected_features.extend(color_features)

            # 3. å½¢çŠ¶ç‰¹å¾æ£€æµ‹ï¼ˆä»…å½“æŸ¥è¯¢æ˜ç¡®åŒ…å«å½¢çŠ¶è¯æ—¶ï¼‰
            shape_features = self._detect_shape_features(image, object_name)
            if shape_features:
                detected_features.extend(shape_features)

            # 4. çº¹ç†ç‰¹å¾æ£€æµ‹ï¼ˆä»…ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼‰
            texture_features = self._detect_texture_features(image, object_name)
            if texture_features:
                detected_features.extend(texture_features)

            # æ›´ä¸¥æ ¼çš„åŒ¹é…æ¡ä»¶ï¼šå¿…é¡»æœ‰æ˜ç¡®çš„ç‰¹å¾åŒ¹é…
            if detected_features:
                is_match = self._validate_opencv_feature_match(object_name, detected_features)
                if is_match:
                    return {
                        'is_match': True,
                        'detected_objects': detected_features,
                        'message': f'OpenCVæ£€æµ‹åˆ°åŒ¹é…ç‰¹å¾: {", ".join(detected_features)}'
                    }

            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç›¸å…³ç‰¹å¾ï¼Œè¿”å›å¤±è´¥
            return {
                'is_match': False,
                'detected_objects': detected_features,
                'message': f'OpenCVæœªæ£€æµ‹åˆ°ä¸"{object_name}"ç›¸å…³çš„ç‰¹å¾ã€‚æ£€æµ‹åˆ°çš„ç‰¹å¾: {", ".join(detected_features)}' if detected_features else f'OpenCVæœªæ£€æµ‹åˆ°ä¸"{object_name}"ç›¸å…³çš„ä»»ä½•ç‰¹å¾'
            }

        except Exception as e:
            return {
                'is_match': False,
                'detected_objects': [],
                'message': f'OpenCVç‰¹å¾éªŒè¯å‡ºé”™: {str(e)}'
            }

    def _check_object_match(self, query, detected_objects):
        """æ£€æŸ¥æŸ¥è¯¢å¯¹è±¡æ˜¯å¦ä¸æ£€æµ‹åˆ°çš„å¯¹è±¡åŒ¹é… - å¢å¼ºç‰ˆåŒ¹é…é€»è¾‘"""
        if not detected_objects:
            return False

        query_lower = query.lower().strip()
        detected_lower = [obj.lower().strip() for obj in detected_objects]

        # 1. ç›´æ¥åŒ¹é…
        if query_lower in detected_lower:
            return True

        # 2. éƒ¨åˆ†åŒ¹é…ï¼ˆæ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼‰
        for detected in detected_lower:
            # åªæœ‰å½“æŸ¥è¯¢è¯æ˜¯æ£€æµ‹åˆ°çš„å¯¹è±¡çš„å®Œæ•´å­ä¸²ï¼Œæˆ–è€…æ£€æµ‹åˆ°çš„å¯¹è±¡æ˜¯æŸ¥è¯¢è¯çš„å®Œæ•´å­ä¸²æ—¶æ‰åŒ¹é…
            if (len(query_lower) >= 2 and query_lower in detected) or (len(detected) >= 2 and detected in query_lower):
                return True

        # 3. æ‰©å±•çš„åŒä¹‰è¯åŒ¹é…ï¼ˆå¤§å¹…æ‰©å±•æ˜ å°„è¡¨ï¼‰
        synonym_map = {
            # äººç±»ç›¸å…³
            'äºº': ['person', 'people', 'human', 'man', 'woman', 'boy', 'girl', 'child'],
            'äººç‰©': ['person', 'people', 'human', 'man', 'woman'],
            'ç”·äºº': ['man', 'person'],
            'å¥³äºº': ['woman', 'person'],
            'å°å­©': ['child', 'boy', 'girl', 'person'],
            'å„¿ç«¥': ['child', 'boy', 'girl', 'person'],

            # åŠ¨ç‰©ç›¸å…³ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
            'ç‹—': ['dog', 'puppy'],
            'å°ç‹—': ['dog', 'puppy'],
            'çŠ¬': ['dog'],
            'çŒ«': ['cat', 'kitten'],
            'å°çŒ«': ['cat', 'kitten'],
            'çŒ«å’ª': ['cat', 'kitten'],
            'é¸Ÿ': ['bird'],
            'å°é¸Ÿ': ['bird'],
            'é©¬': ['horse'],
            'ç‰›': ['cow'],
            'ç¾Š': ['sheep'],
            'å¤§è±¡': ['elephant'],
            'è±¡': ['elephant'],
            'æ–‘é©¬': ['zebra'],
            'é•¿é¢ˆé¹¿': ['giraffe'],
            'é¹¿': ['giraffe'],
            'ç‹®å­': ['lion'],
            'è€è™': ['tiger'],
            'è™': ['tiger'],
            'ç†Š': ['bear'],
            'çŒ´å­': ['monkey'],
            'çŒ´': ['monkey'],
            'å…”å­': ['rabbit'],
            'å…”': ['rabbit'],
            'è€é¼ ': ['mouse'],
            'é¼ ': ['mouse'],
            'åŠ¨ç‰©': ['animal', 'dog', 'cat', 'bird', 'horse', 'cow', 'sheep', 'elephant', 'bear', 'zebra', 'giraffe', 'lion', 'tiger', 'monkey', 'rabbit', 'mouse'],

            # äº¤é€šå·¥å…·ç›¸å…³
            'æ±½è½¦': ['car', 'vehicle', 'automobile'],
            'è½¦': ['car', 'vehicle', 'automobile', 'truck', 'bus'],
            'è½¿è½¦': ['car', 'automobile'],
            'å°è½¦': ['car'],
            'å¡è½¦': ['truck'],
            'è´§è½¦': ['truck'],
            'å¤§è½¦': ['truck', 'bus'],
            'å…¬äº¤è½¦': ['bus'],
            'å·´å£«': ['bus'],
            'è‡ªè¡Œè½¦': ['bicycle', 'bike'],
            'å•è½¦': ['bicycle', 'bike'],
            'è„šè¸è½¦': ['bicycle'],
            'æ‘©æ‰˜è½¦': ['motorcycle'],
            'æœºè½¦': ['motorcycle'],
            'é£æœº': ['airplane', 'aircraft'],
            'å®¢æœº': ['airplane'],
            'èˆ¹': ['boat', 'ship'],
            'å°èˆ¹': ['boat'],
            'ç«è½¦': ['train'],
            'äº¤é€šå·¥å…·': ['car', 'truck', 'bus', 'motorcycle', 'bicycle', 'airplane', 'boat', 'train'],

            # ç‰©å“ç›¸å…³ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
            'ç“¶å­': ['bottle'],
            'æ°´ç“¶': ['bottle'],
            'æ¤…å­': ['chair'],
            'åº§æ¤…': ['chair'],
            'æ¡Œå­': ['table', 'desk'],
            'é¤æ¡Œ': ['table'],
            'ä¹¦': ['book'],
            'ä¹¦æœ¬': ['book'],
            'ç”µè„‘': ['laptop', 'computer'],
            'ç¬”è®°æœ¬ç”µè„‘': ['laptop'],
            'æ‰‹æœº': ['phone', 'cell phone'],
            'ç”µè¯': ['phone'],
            'æ—¶é’Ÿ': ['clock'],
            'é’Ÿè¡¨': ['clock'],
            'é”®ç›˜': ['keyboard'],
            'é¼ æ ‡': ['mouse'],
            'æ˜¾ç¤ºå™¨': ['monitor'],
            'å±å¹•': ['monitor', 'tv'],
            'ç”µè§†': ['tv'],
            'å‰ªåˆ€': ['scissors'],
            'å‰ªå­': ['scissors'],
            'æ³°è¿ªç†Š': ['teddy bear'],
            'ç©å…·ç†Š': ['teddy bear'],
            'ç†Šå¨ƒå¨ƒ': ['teddy bear'],
            'å¹é£æœº': ['hair drier'],
            'ç”µå¹é£': ['hair drier'],
            'ç‰™åˆ·': ['toothbrush'],
            'ç”µåŠ¨ç‰™åˆ·': ['toothbrush'],
            'é›¨ä¼': ['umbrella'],
            'ä¼': ['umbrella'],
            'æ‰‹æåŒ…': ['handbag'],
            'åŒ…': ['handbag', 'backpack'],
            'èƒŒåŒ…': ['backpack'],
            'è¡Œæç®±': ['suitcase'],
            'ç®±å­': ['suitcase'],

            # è‡ªç„¶ç‰©ä½“ç›¸å…³
            'èŠ±': ['flower'],
            'èŠ±æœµ': ['flower'],
            'æ ‘': ['tree'],
            'æ ‘æœ¨': ['tree'],
            'æˆ¿å­': ['house', 'building'],
            'å»ºç­‘': ['building', 'house'],
            'æ¥¼æˆ¿': ['building'],

            # é£Ÿç‰©ç›¸å…³ï¼ˆæ–°å¢ï¼‰
            'è‹¹æœ': ['apple'],
            'é¦™è•‰': ['banana'],
            'æ©™å­': ['orange'],
            'æ©˜å­': ['orange'],
            'è›‹ç³•': ['cake'],
            'é¢åŒ…': ['bread'],
            'ä¸‰æ˜æ²»': ['sandwich'],
            'çƒ­ç‹—': ['hot dog'],
            'æŠ«è¨': ['pizza'],
            'ç”œç”œåœˆ': ['donut'],
            'èƒ¡èåœ': ['carrot'],
            'èåœ': ['carrot'],

            # è¿åŠ¨ç›¸å…³
            'çƒ': ['ball', 'sports ball'],
            'è¶³çƒ': ['sports ball'],
            'ç¯®çƒ': ['sports ball'],
            'ç½‘çƒ': ['sports ball', 'tennis racket'],
            'ç½‘çƒæ‹': ['tennis racket'],
            'æ£’çƒæ£’': ['baseball bat'],
            'æ»‘æ¿': ['skateboard'],
            'æ»‘é›ªæ¿': ['skis'],
            'é£ç­': ['kite'],

            # è‹±æ–‡åˆ°ä¸­æ–‡çš„åå‘æ˜ å°„ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
            'person': ['äºº', 'äººç‰©', 'äººç±»'],
            'dog': ['ç‹—', 'å°ç‹—', 'çŠ¬'],
            'cat': ['çŒ«', 'å°çŒ«', 'çŒ«å’ª'],
            'car': ['æ±½è½¦', 'è½¦', 'è½¿è½¦', 'å°è½¦'],
            'truck': ['å¡è½¦', 'è´§è½¦', 'å¤§è½¦'],
            'bus': ['å…¬äº¤è½¦', 'å·´å£«', 'å¤§è½¦'],
            'bird': ['é¸Ÿ', 'å°é¸Ÿ'],
            'horse': ['é©¬'],
            'cow': ['ç‰›'],
            'sheep': ['ç¾Š'],
            'elephant': ['å¤§è±¡', 'è±¡'],
            'zebra': ['æ–‘é©¬'],
            'giraffe': ['é•¿é¢ˆé¹¿', 'é¹¿'],
            'lion': ['ç‹®å­'],
            'tiger': ['è€è™', 'è™'],
            'bear': ['ç†Š'],
            'monkey': ['çŒ´å­', 'çŒ´'],
            'rabbit': ['å…”å­', 'å…”'],
            'mouse': ['è€é¼ ', 'é¼ '],
            'bicycle': ['è‡ªè¡Œè½¦', 'å•è½¦', 'è„šè¸è½¦'],
            'motorcycle': ['æ‘©æ‰˜è½¦', 'æœºè½¦'],
            'airplane': ['é£æœº', 'å®¢æœº'],
            'boat': ['èˆ¹', 'å°èˆ¹'],
            'train': ['ç«è½¦'],
            'bottle': ['ç“¶å­', 'æ°´ç“¶'],
            'chair': ['æ¤…å­', 'åº§æ¤…'],
            'table': ['æ¡Œå­', 'é¤æ¡Œ'],
            'book': ['ä¹¦', 'ä¹¦æœ¬'],
            'laptop': ['ç”µè„‘', 'ç¬”è®°æœ¬ç”µè„‘'],
            'phone': ['æ‰‹æœº', 'ç”µè¯'],
            'clock': ['æ—¶é’Ÿ', 'é’Ÿè¡¨'],
            'keyboard': ['é”®ç›˜'],
            'mouse': ['é¼ æ ‡'],
            'monitor': ['æ˜¾ç¤ºå™¨', 'å±å¹•'],
            'tv': ['ç”µè§†', 'å±å¹•'],
            'scissors': ['å‰ªåˆ€', 'å‰ªå­'],
            'teddy bear': ['æ³°è¿ªç†Š', 'ç©å…·ç†Š', 'ç†Šå¨ƒå¨ƒ'],
            'hair drier': ['å¹é£æœº', 'ç”µå¹é£'],
            'toothbrush': ['ç‰™åˆ·', 'ç”µåŠ¨ç‰™åˆ·'],
            'umbrella': ['é›¨ä¼', 'ä¼'],
            'handbag': ['æ‰‹æåŒ…', 'åŒ…'],
            'backpack': ['èƒŒåŒ…', 'åŒ…'],
            'suitcase': ['è¡Œæç®±', 'ç®±å­'],
            'flower': ['èŠ±', 'èŠ±æœµ'],
            'tree': ['æ ‘', 'æ ‘æœ¨'],
            'house': ['æˆ¿å­'],
            'building': ['å»ºç­‘', 'æ¥¼æˆ¿'],
            'apple': ['è‹¹æœ'],
            'banana': ['é¦™è•‰'],
            'orange': ['æ©™å­', 'æ©˜å­'],
            'cake': ['è›‹ç³•'],
            'bread': ['é¢åŒ…'],
            'sandwich': ['ä¸‰æ˜æ²»'],
            'hot dog': ['çƒ­ç‹—'],
            'pizza': ['æŠ«è¨'],
            'donut': ['ç”œç”œåœˆ'],
            'carrot': ['èƒ¡èåœ', 'èåœ'],
            'ball': ['çƒ'],
            'sports ball': ['è¶³çƒ', 'ç¯®çƒ', 'çƒ'],
            'tennis racket': ['ç½‘çƒæ‹'],
            'baseball bat': ['æ£’çƒæ£’'],
            'skateboard': ['æ»‘æ¿'],
            'skis': ['æ»‘é›ªæ¿'],
            'kite': ['é£ç­']
        }

        # 4. æ£€æŸ¥åŒä¹‰è¯åŒ¹é…
        for key, synonyms in synonym_map.items():
            if key == query_lower:
                # æŸ¥è¯¢è¯åœ¨åŒä¹‰è¯æ˜ å°„çš„é”®ä¸­
                for synonym in synonyms:
                    if synonym in detected_lower:
                        return True
            elif query_lower in synonyms:
                # æŸ¥è¯¢è¯åœ¨åŒä¹‰è¯åˆ—è¡¨ä¸­
                if key in detected_lower:
                    return True
                # æ£€æŸ¥å…¶ä»–åŒä¹‰è¯
                for synonym in synonyms:
                    if synonym in detected_lower:
                        return True

        # 5. æ¨¡ç³ŠåŒ¹é…ï¼ˆä»…å¯¹é•¿åº¦å¤§äº3çš„è¯è¿›è¡Œï¼‰
        if len(query_lower) > 3:
            for detected in detected_lower:
                if len(detected) > 3:
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦é‡å ï¼‰
                    common_chars = set(query_lower) & set(detected)
                    similarity = len(common_chars) / max(len(query_lower), len(detected))
                    if similarity > 0.6:  # 60%ä»¥ä¸Šçš„å­—ç¬¦é‡å 
                        return True

        # 6. è¯­ä¹‰ç»„åŒ¹é…ï¼ˆæ–°å¢ï¼‰
        semantic_groups = {
            'animals': ['dog', 'cat', 'bird', 'horse', 'cow', 'sheep', 'elephant', 'zebra', 'giraffe', 'lion', 'tiger', 'bear', 'monkey', 'rabbit', 'mouse'],
            'vehicles': ['car', 'truck', 'bus', 'motorcycle', 'bicycle', 'airplane', 'boat', 'train'],
            'furniture': ['chair', 'table', 'sofa', 'bed'],
            'electronics': ['laptop', 'phone', 'tv', 'keyboard', 'mouse', 'monitor'],
            'food': ['apple', 'banana', 'orange', 'cake', 'bread', 'sandwich', 'hot dog', 'pizza', 'donut', 'carrot'],
            'sports': ['ball', 'sports ball', 'tennis racket', 'baseball bat', 'skateboard', 'skis', 'kite']
        }

        # æ£€æŸ¥æ˜¯å¦å±äºåŒä¸€è¯­ä¹‰ç»„
        query_group = None
        detected_groups = set()

        # æ‰¾åˆ°æŸ¥è¯¢è¯æ‰€å±çš„è¯­ä¹‰ç»„
        for group_name, items in semantic_groups.items():
            if any(synonym in query_lower for synonym in items):
                query_group = group_name
                break
            # æ£€æŸ¥ä¸­æ–‡åŒä¹‰è¯
            for item in items:
                if item in synonym_map and query_lower in synonym_map[item]:
                    query_group = group_name
                    break

        # æ‰¾åˆ°æ£€æµ‹å¯¹è±¡æ‰€å±çš„è¯­ä¹‰ç»„
        for detected in detected_lower:
            for group_name, items in semantic_groups.items():
                if detected in items:
                    detected_groups.add(group_name)

        # å¦‚æœå±äºåŒä¸€è¯­ä¹‰ç»„ï¼Œåˆ™è®¤ä¸ºå¯èƒ½åŒ¹é…ï¼ˆé™ä½åŒ¹é…ä¸¥æ ¼åº¦ï¼‰
        if query_group and query_group in detected_groups:
            # å¯¹äºè¯­ä¹‰ç»„åŒ¹é…ï¼Œæˆ‘ä»¬å¯ä»¥ç»™å‡ºæç¤ºè€Œä¸æ˜¯ç›´æ¥åŒ¹é…
            # è¿™é‡Œæš‚æ—¶è¿”å›Falseï¼Œè®©ä¸Šå±‚é€»è¾‘å¤„ç†
            pass

        return False

    def _detect_color_features(self, image, object_name):
        """æ£€æµ‹é¢œè‰²ç‰¹å¾"""
        features = []

        # é¢œè‰²å…³é”®è¯æ˜ å°„
        color_keywords = {
            'çº¢': [(0, 0, 100), (10, 255, 255), (170, 255, 255), (180, 255, 255)],
            'ç»¿': [(40, 40, 40), (80, 255, 255)],
            'è“': [(100, 40, 40), (130, 255, 255)],
            'é»„': [(20, 40, 40), (40, 255, 255)],
            'ç™½': [(0, 0, 200), (180, 30, 255)],
            'é»‘': [(0, 0, 0), (180, 255, 50)]
        }

        # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«é¢œè‰²è¯
        for color_name, hsv_ranges in color_keywords.items():
            if color_name in object_name:
                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

                # åˆ›å»ºé¢œè‰²æ©ç 
                mask = np.zeros(hsv.shape[:2], dtype=np.uint8)
                for i in range(0, len(hsv_ranges), 2):
                    lower = np.array(hsv_ranges[i])
                    upper = np.array(hsv_ranges[i+1])
                    color_mask = cv2.inRange(hsv, lower, upper)
                    mask = cv2.bitwise_or(mask, color_mask)

                # æ£€æŸ¥é¢œè‰²åŒºåŸŸå¤§å°
                color_area = cv2.countNonZero(mask)
                total_area = image.shape[0] * image.shape[1]

                if color_area > total_area * 0.05:  # é¢œè‰²åŒºåŸŸå 5%ä»¥ä¸Š
                    features.append(f'{color_name}è‰²åŒºåŸŸ')

        return features

    def _detect_shape_features(self, image, object_name):
        """æ£€æµ‹å½¢çŠ¶ç‰¹å¾"""
        features = []

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)

        # æ£€æµ‹åœ†å½¢
        if any(keyword in object_name for keyword in ['åœ†', 'çƒ', 'circle', 'ball']):
            circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20,
                                     param1=50, param2=30, minRadius=10, maxRadius=100)
            if circles is not None:
                features.append('åœ†å½¢å¯¹è±¡')

        # æ£€æµ‹ç›´çº¿å’ŒçŸ©å½¢
        if any(keyword in object_name for keyword in ['æ–¹', 'çŸ©å½¢', 'ç›´çº¿', 'square', 'rectangle', 'line']):
            edges = cv2.Canny(blurred, 50, 150)
            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)
            if lines is not None and len(lines) > 4:
                features.append('çŸ©å½¢/ç›´çº¿ç»“æ„')

        return features

    def _detect_texture_features(self, image, object_name):
        """æ£€æµ‹çº¹ç†ç‰¹å¾"""
        features = []

        # ç®€å•çš„çº¹ç†æ£€æµ‹
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # è®¡ç®—å›¾åƒçš„æ ‡å‡†å·®ï¼ˆçº¹ç†å¤æ‚åº¦æŒ‡æ ‡ï¼‰
        std_dev = np.std(gray)

        if std_dev > 50:  # é«˜çº¹ç†å¤æ‚åº¦
            features.append('å¤æ‚çº¹ç†')
        elif std_dev < 20:  # ä½çº¹ç†å¤æ‚åº¦
            features.append('å¹³æ»‘è¡¨é¢')

        return features

    def _detect_face_features(self, image):
        """æ£€æµ‹äººè„¸ç‰¹å¾"""
        features = []

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # ä½¿ç”¨Haarçº§è”æ£€æµ‹äººè„¸
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        if len(faces) > 0:
            features.append('äººè„¸')

        # æ£€æµ‹çœ¼ç›
        eyes = self.eye_cascade.detectMultiScale(gray, 1.1, 4)
        if len(eyes) > 0:
            features.append('çœ¼éƒ¨ç‰¹å¾')

        return features

    def _validate_opencv_feature_match(self, object_name, detected_features):
        """éªŒè¯OpenCVæ£€æµ‹åˆ°çš„ç‰¹å¾æ˜¯å¦ä¸æŸ¥è¯¢å¯¹è±¡åŒ¹é… - æ›´ä¸¥æ ¼çš„åŒ¹é…è§„åˆ™"""
        if not detected_features:
            return False

        object_name_lower = object_name.lower().strip()

        # ä¸¥æ ¼çš„ç‰¹å¾åŒ¹é…è§„åˆ™
        feature_matches = {
            # é¢œè‰²åŒ¹é… - åªæœ‰æ˜ç¡®åŒ…å«é¢œè‰²è¯æ‰åŒ¹é…
            'çº¢': ['çº¢è‰²åŒºåŸŸ'],
            'çº¢è‰²': ['çº¢è‰²åŒºåŸŸ'],
            'ç»¿': ['ç»¿è‰²åŒºåŸŸ'],
            'ç»¿è‰²': ['ç»¿è‰²åŒºåŸŸ'],
            'è“': ['è“è‰²åŒºåŸŸ'],
            'è“è‰²': ['è“è‰²åŒºåŸŸ'],
            'é»„': ['é»„è‰²åŒºåŸŸ'],
            'é»„è‰²': ['é»„è‰²åŒºåŸŸ'],
            'ç™½': ['ç™½è‰²åŒºåŸŸ'],
            'ç™½è‰²': ['ç™½è‰²åŒºåŸŸ'],
            'é»‘': ['é»‘è‰²åŒºåŸŸ'],
            'é»‘è‰²': ['é»‘è‰²åŒºåŸŸ'],

            # å½¢çŠ¶åŒ¹é… - åªæœ‰æ˜ç¡®åŒ…å«å½¢çŠ¶è¯æ‰åŒ¹é…
            'åœ†': ['åœ†å½¢å¯¹è±¡'],
            'åœ†å½¢': ['åœ†å½¢å¯¹è±¡'],
            'çƒ': ['åœ†å½¢å¯¹è±¡'],
            'æ–¹': ['çŸ©å½¢/ç›´çº¿ç»“æ„'],
            'æ–¹å½¢': ['çŸ©å½¢/ç›´çº¿ç»“æ„'],
            'çŸ©å½¢': ['çŸ©å½¢/ç›´çº¿ç»“æ„'],
            'æ­£æ–¹å½¢': ['çŸ©å½¢/ç›´çº¿ç»“æ„'],
            'é•¿æ–¹å½¢': ['çŸ©å½¢/ç›´çº¿ç»“æ„'],

            # äººè„¸åŒ¹é… - äººç›¸å…³çš„æŸ¥è¯¢
            'äºº': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'è„¸': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'äººè„¸': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'å¤´': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'ç”·äºº': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'å¥³äºº': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'å°å­©': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'å„¿ç«¥': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'äººç‰©': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'person': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'face': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'head': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'man': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'woman': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'child': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'boy': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾'],
            'girl': ['äººè„¸', 'çœ¼éƒ¨ç‰¹å¾']
        }

        # æ£€æŸ¥æ˜¯å¦æœ‰ç²¾ç¡®åŒ¹é…çš„ç‰¹å¾
        for keyword, expected_features in feature_matches.items():
            if keyword in object_name_lower:
                for feature in detected_features:
                    if any(expected in feature for expected in expected_features):
                        return True

        # å¯¹äºéç‰¹å®šç‰¹å¾æŸ¥è¯¢ï¼Œä¸å…è®¸é€šè¿‡OpenCVéªŒè¯
        # è¿™æ ·å¯ä»¥é¿å…è¯¯åˆ¤ï¼Œè®©YOLOæˆ–Geminiæ¥å¤„ç†å¤æ‚å¯¹è±¡è¯†åˆ«
        return False

    def _detect_faces_haar(self, image):
        """ä½¿ç”¨ Haar Cascade æ£€æµ‹äººè„¸"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        detected_objects = []
        height, width = image.shape[:2]

        # æ£€æµ‹äººè„¸
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        for (x, y, w, h) in faces:
            ymin = y / height
            xmin = x / width
            ymax = (y + h) / height
            xmax = (x + w) / width

            detected_objects.append({
                'label': 'äººè„¸',
                'confidence': 0.8,
                'bbox': [ymin, xmin, ymax, xmax],
                'method': 'Haar Cascade'
            })

        return detected_objects

    def _detect_contours(self, image, object_name='å¯¹è±¡'):
        """ä½¿ç”¨è½®å»“æ£€æµ‹å¯¹è±¡"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # å¤šç§é¢„å¤„ç†æ–¹æ³•
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)

        # è‡ªé€‚åº”é˜ˆå€¼
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)

        # å½¢æ€å­¦æ“ä½œ
        kernel = np.ones((3, 3), np.uint8)
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)

        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        detected_objects = []
        height, width = image.shape[:2]

        # æŒ‰é¢ç§¯æ’åºï¼Œå–æœ€å¤§çš„å‡ ä¸ª
        contours = sorted(contours, key=cv2.contourArea, reverse=True)

        for i, contour in enumerate(contours[:5]):  # æœ€å¤šå–5ä¸ªæœ€å¤§çš„è½®å»“
            area = cv2.contourArea(contour)
            if area > 1000:  # è¿‡æ»¤å¤ªå°çš„è½®å»“
                x, y, w, h = cv2.boundingRect(contour)

                # è®¡ç®—è½®å»“çš„ç´§å¯†åº¦ï¼ˆç”¨äºè¯„ä¼°å¯¹è±¡è´¨é‡ï¼‰
                perimeter = cv2.arcLength(contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    confidence = min(0.9, max(0.3, circularity))
                else:
                    confidence = 0.5

                # è½¬æ¢ä¸ºå½’ä¸€åŒ–åæ ‡
                ymin = y / height
                xmin = x / width
                ymax = (y + h) / height
                xmax = (x + w) / width

                detected_objects.append({
                    'label': f'{object_name}_{i+1}',
                    'confidence': confidence,
                    'bbox': [ymin, xmin, ymax, xmax],
                    'method': 'Contour Detection'
                })

        return detected_objects

    def _detect_by_color(self, image, object_name='å¯¹è±¡'):
        """ä½¿ç”¨é¢œè‰²åˆ†å‰²æ£€æµ‹å¯¹è±¡"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        height, width = image.shape[:2]
        detected_objects = []

        # å®šä¹‰å¤šä¸ªé¢œè‰²èŒƒå›´
        color_ranges = [
            # çº¢è‰²
            ([0, 50, 50], [10, 255, 255]),
            ([170, 50, 50], [180, 255, 255]),
            # ç»¿è‰²
            ([40, 50, 50], [80, 255, 255]),
            # è“è‰²
            ([100, 50, 50], [130, 255, 255]),
            # é»„è‰²
            ([20, 50, 50], [40, 255, 255]),
        ]

        for i, (lower, upper) in enumerate(color_ranges):
            lower = np.array(lower)
            upper = np.array(upper)

            # åˆ›å»ºæ©ç 
            mask = cv2.inRange(hsv, lower, upper)

            # å½¢æ€å­¦æ“ä½œ
            kernel = np.ones((5, 5), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for j, contour in enumerate(contours):
                area = cv2.contourArea(contour)
                if area > 500:
                    x, y, w, h = cv2.boundingRect(contour)

                    ymin = y / height
                    xmin = x / width
                    ymax = (y + h) / height
                    xmax = (x + w) / width

                    color_names = ['çº¢è‰²', 'çº¢è‰²', 'ç»¿è‰²', 'è“è‰²', 'é»„è‰²']

                    detected_objects.append({
                        'label': f'{color_names[i]}{object_name}_{j+1}',
                        'confidence': 0.7,
                        'bbox': [ymin, xmin, ymax, xmax],
                        'method': 'Color Segmentation'
                    })

        return detected_objects[:3]  # æœ€å¤šè¿”å›3ä¸ªå¯¹è±¡

    def _detect_by_edges(self, image, object_name='å¯¹è±¡'):
        """ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹å¯¹è±¡"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # å¤šå°ºåº¦è¾¹ç¼˜æ£€æµ‹
        edges1 = cv2.Canny(gray, 50, 150)
        edges2 = cv2.Canny(gray, 100, 200)

        # åˆå¹¶è¾¹ç¼˜
        edges = cv2.bitwise_or(edges1, edges2)

        # è†¨èƒ€æ“ä½œè¿æ¥æ–­å¼€çš„è¾¹ç¼˜
        kernel = np.ones((3, 3), np.uint8)
        edges = cv2.dilate(edges, kernel, iterations=1)

        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        detected_objects = []
        height, width = image.shape[:2]

        # æŒ‰é¢ç§¯æ’åº
        contours = sorted(contours, key=cv2.contourArea, reverse=True)

        for i, contour in enumerate(contours[:3]):  # æœ€å¤šå–3ä¸ª
            area = cv2.contourArea(contour)
            if area > 800:
                x, y, w, h = cv2.boundingRect(contour)

                ymin = y / height
                xmin = x / width
                ymax = (y + h) / height
                xmax = (x + w) / width

                detected_objects.append({
                    'label': f'è¾¹ç¼˜{object_name}_{i+1}',
                    'confidence': 0.6,
                    'bbox': [ymin, xmin, ymax, xmax],
                    'method': 'Edge Detection'
                })

        return detected_objects

    def _draw_opencv_bbox(self, image_path, bbox_coords, output_path, label, method, color_index=0):
        """ç»˜åˆ¶ OpenCV æ£€æµ‹çš„è¾¹ç•Œæ¡† - æ”¯æŒä¸åŒé¢œè‰²"""
        image = cv2.imread(image_path)
        height, width = image.shape[:2]

        ymin, xmin, ymax, xmax = bbox_coords
        x1 = int(xmin * width)
        y1 = int(ymin * height)
        x2 = int(xmax * width)
        y2 = int(ymax * height)

        # é¢œè‰²è°ƒè‰²æ¿
        colors = [
            (255, 0, 0),    # çº¢è‰²
            (0, 255, 0),    # ç»¿è‰²
            (0, 0, 255),    # è“è‰²
            (255, 255, 0),  # é’è‰²
            (255, 0, 255),  # æ´‹çº¢è‰²
            (0, 255, 255),  # é»„è‰²
            (128, 0, 255),  # ç´«è‰²
            (255, 128, 0),  # æ©™è‰²
        ]

        color = colors[color_index % len(colors)]

        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 3)

        # æ·»åŠ æ ‡ç­¾èƒŒæ™¯å’Œæ–‡æœ¬
        label_text = f"{label} ({method})"
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        font_thickness = 2
        (text_width, text_height), baseline = cv2.getTextSize(label_text, font, font_scale, font_thickness)

        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        cv2.rectangle(image, (x1, y1 - text_height - 10), (x1 + text_width + 10, y1), color, -1)

        # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬ï¼ˆç™½è‰²ï¼‰
        cv2.putText(image, label_text, (x1 + 5, y1 - 5), font, font_scale, (255, 255, 255), font_thickness)

        cv2.imwrite(output_path, image)
        return output_path

    def _draw_all_opencv_bboxes(self, image_path, detected_objects, output_path):
        """ç»˜åˆ¶æ‰€æœ‰ OpenCV æ£€æµ‹çš„è¾¹ç•Œæ¡† - ä½¿ç”¨ä¸åŒé¢œè‰²åŒºåˆ†å¯¹è±¡"""
        image = cv2.imread(image_path)
        height, width = image.shape[:2]

        # æ‰©å±•é¢œè‰²è°ƒè‰²æ¿ï¼Œä½¿ç”¨æ›´é²œæ˜çš„é¢œè‰²
        colors = [
            (255, 0, 0),    # çº¢è‰²
            (0, 255, 0),    # ç»¿è‰²
            (0, 0, 255),    # è“è‰²
            (255, 255, 0),  # é’è‰²
            (255, 0, 255),  # æ´‹çº¢è‰²
            (0, 255, 255),  # é»„è‰²
            (128, 0, 255),  # ç´«è‰²
            (255, 128, 0),  # æ©™è‰²
            (0, 128, 255),  # å¤©è“è‰²
            (128, 255, 0),  # é’ç»¿è‰²
            (255, 0, 128),  # ç²‰çº¢è‰²
            (0, 255, 128),  # æ˜¥ç»¿è‰²
        ]

        for i, obj in enumerate(detected_objects):
            bbox = obj['bbox']
            label = obj['label']
            method = obj['method']
            confidence = obj.get('confidence', 0.0)

            ymin, xmin, ymax, xmax = bbox
            x1 = int(xmin * width)
            y1 = int(ymin * height)
            x2 = int(xmax * width)
            y2 = int(ymax * height)

            # ä½¿ç”¨ä¸åŒé¢œè‰²
            color = colors[i % len(colors)]

            # ç»˜åˆ¶è¾¹ç•Œæ¡†ï¼Œçº¿æ¡ç²—ç»†æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´
            thickness = max(2, int(confidence * 4)) if confidence > 0 else 3
            cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)

            # æ·»åŠ æ ‡ç­¾èƒŒæ™¯
            label_text = f"{label} ({confidence:.2f})" if confidence > 0 else f"{label} ({method})"

            # è®¡ç®—æ–‡æœ¬å°ºå¯¸
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            font_thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(label_text, font, font_scale, font_thickness)

            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(image, (x1, y1 - text_height - 10), (x1 + text_width + 10, y1), color, -1)

            # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬ï¼ˆç™½è‰²ï¼‰
            cv2.putText(image, label_text, (x1 + 5, y1 - 5), font, font_scale, (255, 255, 255), font_thickness)

            # åœ¨å³ä¸Šè§’æ·»åŠ å¯¹è±¡ç¼–å·
            number_text = f"#{i+1}"
            cv2.putText(image, number_text, (x2 - 30, y1 + 20), font, 0.5, color, 2)

        cv2.imwrite(output_path, image)
        return output_path

    def segment_image_opencv(self, file=None, image_data=None, method='contour_mask', object_name = ""):
        """ä½¿ç”¨ OpenCV è¿›è¡Œå›¾åƒåˆ†å‰²"""
        try:
            # å¤„ç†æ–‡ä»¶è¾“å…¥
            if image_data:
                image_data_clean = image_data.split(',')[1] if ',' in image_data else image_data
                image_bytes = base64.b64decode(image_data_clean)
                with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                    tmp_file.write(image_bytes)
                    filepath = tmp_file.name
            else:
                if not file or not hasattr(file, 'filename') or file.filename == '':
                    return {'success': False, 'error': 'æœªé€‰æ‹©æ–‡ä»¶'}, 400
                if not allowed_file(file.filename):
                    return {'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶ç±»å‹'}, 400

                # æ£€æŸ¥æ˜¯å¦æ˜¯å·²ç»å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæµ‹è¯•ï¼‰
                if hasattr(file, 'filepath') and os.path.exists(file.filepath):
                    filepath = file.filepath
                else:
                    # æ ‡å‡†çš„Flaskæ–‡ä»¶ä¸Šä¼ å¯¹è±¡
                    filepath = save_uploaded_file(file, current_app.config['UPLOAD_FOLDER'])

            # è¯»å–å›¾åƒ - ä½¿ç”¨å¤šç§æ–¹æ³•ç¡®ä¿èƒ½å¤Ÿè¯»å–
            image = None

            # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨cv2.imread
            try:
                image = cv2.imread(filepath)
            except Exception as e:
                print(f"cv2.imread å¤±è´¥: {e}")

            # æ–¹æ³•2: å¦‚æœç›´æ¥è¯»å–å¤±è´¥ï¼Œä½¿ç”¨PILè½¬æ¢
            if image is None:
                try:
                    from PIL import Image as PILImage
                    pil_image = PILImage.open(filepath)
                    # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœæ˜¯RGBAï¼‰
                    if pil_image.mode == 'RGBA':
                        pil_image = pil_image.convert('RGB')
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    image_array = np.array(pil_image)
                    # PILä½¿ç”¨RGBï¼ŒOpenCVä½¿ç”¨BGRï¼Œéœ€è¦è½¬æ¢
                    image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
                    print("ä½¿ç”¨PILæˆåŠŸè¯»å–å›¾åƒ")
                except Exception as e:
                    print(f"PILè¯»å–å¤±è´¥: {e}")

            # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨numpyç›´æ¥è¯»å–
            if image is None:
                try:
                    # è¯»å–æ–‡ä»¶å­—èŠ‚
                    with open(filepath, 'rb') as f:
                        file_bytes = f.read()

                    # ä½¿ç”¨numpyå’Œcv2è§£ç 
                    nparr = np.frombuffer(file_bytes, np.uint8)
                    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                    print("ä½¿ç”¨numpyæˆåŠŸè¯»å–å›¾åƒ")
                except Exception as e:
                    print(f"numpyè¯»å–å¤±è´¥: {e}")

            if image is None:
                error_msg = f'æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {filepath}ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚'
                print(error_msg)
                return {'success': False, 'error': error_msg}, 400

            # ä¸Geminiåˆ†å‰²æœåŠ¡ä¿æŒä¸€è‡´çš„è¾“å…¥éªŒè¯
            if not object_name or not object_name.strip():
                return {'success': False, 'error': 'è¯·è¾“å…¥è¦åˆ†å‰²çš„å¯¹è±¡åç§°'}, 400

            # è¿›è¡Œå†…å®¹éªŒè¯ï¼Œç¡®ä¿å›¾åƒä¸­åŒ…å«æŒ‡å®šå¯¹è±¡
            print(f"OpenCVåˆ†å‰²ï¼šå¼€å§‹éªŒè¯å›¾åƒå†…å®¹æ˜¯å¦åŒ…å« '{object_name.strip()}'")
            validation_result = self._validate_image_content(filepath, object_name.strip())
            if not validation_result['is_match']:
                print(f"OpenCVåˆ†å‰²ï¼šå†…å®¹éªŒè¯å¤±è´¥ï¼Œæœªæ£€æµ‹åˆ° '{object_name.strip()}'")
                return {
                    'success': False,
                    'error': f'æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼š{object_name.strip()}',
                    'message': validation_result['message'],
                    'suggestion': validation_result.get('suggestion', 'è¯·æ£€æŸ¥å›¾åƒå†…å®¹æˆ–ä¿®æ”¹æŸ¥è¯¢è¯æ±‡ã€‚'),
                    'detected_objects': validation_result.get('detected_objects', []),
                    'alternative_queries': validation_result.get('alternative_queries', []),
                    'content_mismatch': True,
                    'user_query': object_name.strip(),
                    'validation_method': validation_result.get('validation_method', 'OpenCVéªŒè¯'),
                    'opencv_limitation': validation_result.get('opencv_limitation', False)
                }, 200  # æ”¹ä¸º200çŠ¶æ€ç ï¼Œè®©å‰ç«¯æ­£ç¡®å¤„ç†å†…å®¹ä¸åŒ¹é…
            else:
                print(f"OpenCVåˆ†å‰²ï¼šå†…å®¹éªŒè¯æˆåŠŸï¼Œä½¿ç”¨æ–¹æ³•ï¼š{validation_result.get('validation_method', 'æœªçŸ¥')}")

            segmented_objects = []
            segment_images = []

            if method == 'contour_mask':
                # ä½¿ç”¨è½®å»“æ©ç åˆ†å‰²ï¼ˆæ¨èï¼‰
                segments = self._contour_mask_segmentation(image, filepath, object_name)
                segmented_objects.extend(segments)

            elif method == 'grabcut':
                # ä½¿ç”¨ GrabCut ç®—æ³•
                segments = self._grabcut_segmentation(image, filepath)
                segmented_objects.extend(segments)

            elif method == 'watershed':
                # ä½¿ç”¨ Watershed ç®—æ³•
                segments = self._watershed_segmentation(image, filepath)
                segmented_objects.extend(segments)

            elif method == 'kmeans':
                # ä½¿ç”¨ K-means èšç±»
                segments = self._kmeans_segmentation(image, filepath)
                segmented_objects.extend(segments)

            # æ”¶é›†åˆ†å‰²å›¾åƒè·¯å¾„
            for i, segment in enumerate(segmented_objects):
                if 'segment_image' in segment and os.path.exists(segment['segment_image']):
                    segment_images.append(segment['segment_image'])
                    print(f"åˆ†å‰²å›¾åƒå·²ä¿å­˜: {segment['segment_image']}")
                else:
                    print(f"åˆ†å‰²å›¾åƒä¸å­˜åœ¨: {segment.get('segment_image', 'N/A')}")

            if segmented_objects:
                return {
                    'success': True,
                    'original_image': filepath,
                    'segmented_objects': segmented_objects,
                    'segment_images': segment_images,
                    'method': f'OpenCV {method}'
                }, 200
            else:
                return {
                    'success': False,
                    'error': f'ä½¿ç”¨ OpenCV {method} æ–¹æ³•æœªèƒ½åˆ†å‰²å‡ºå¯¹è±¡',
                    'method': f'OpenCV {method}'
                }, 200

        except Exception as e:
            error_msg = f'OpenCV åˆ†å‰²å¤±è´¥: {str(e)}'
            print(f"OpenCV åˆ†å‰²é”™è¯¯: {e}")
            return {'success': False, 'error': error_msg}, 500

    def _contour_mask_segmentation(self, image, filepath, object_name='ä¸»è¦å¯¹è±¡'):
        """åŸºäºè½®å»“çš„æ©ç åˆ†å‰² - ç²¾ç¡®åˆ†å‰²å¯¹è±¡è½®å»“"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        height, width = image.shape[:2]

        # å¤šç§é¢„å¤„ç†æ–¹æ³•ç»„åˆ
        # 1. é«˜æ–¯æ¨¡ç³Šå»å™ª
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)

        # 2. å¤šç§é˜ˆå€¼æ–¹æ³•
        # è‡ªé€‚åº”é˜ˆå€¼
        thresh1 = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)

        # Otsué˜ˆå€¼
        _, thresh2 = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # ç»„åˆé˜ˆå€¼ç»“æœ
        thresh = cv2.bitwise_or(thresh1, thresh2)

        # 3. å½¢æ€å­¦æ“ä½œä¼˜åŒ–è½®å»“
        # é—­è¿ç®—ï¼šè¿æ¥æ–­å¼€çš„è½®å»“
        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel_close)

        # å¼€è¿ç®—ï¼šå»é™¤å™ªå£°
        kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel_open)

        # 4. æŸ¥æ‰¾è½®å»“
        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # 5. è½®å»“ç­›é€‰å’Œè´¨é‡è¯„ä¼°
        valid_contours = []
        for contour in contours:
            area = cv2.contourArea(contour)
            perimeter = cv2.arcLength(contour, True)

            # é¢ç§¯ç­›é€‰
            if area < 1000:  # è¿‡æ»¤å¤ªå°çš„è½®å»“
                continue

            # è®¡ç®—è½®å»“è´¨é‡æŒ‡æ ‡
            if perimeter > 0:
                # åœ†å½¢åº¦ï¼šè¶Šæ¥è¿‘1è¶Šåœ†
                circularity = 4 * np.pi * area / (perimeter * perimeter)

                # å‡¸åŒ…æ¯”ç‡ï¼šè½®å»“é¢ç§¯ä¸å…¶å‡¸åŒ…é¢ç§¯çš„æ¯”ç‡
                hull = cv2.convexHull(contour)
                hull_area = cv2.contourArea(hull)
                if hull_area > 0:
                    solidity = area / hull_area
                else:
                    solidity = 0

                # é•¿å®½æ¯”
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = float(w) / h if h > 0 else 0

                # ç»¼åˆè´¨é‡è¯„åˆ†
                quality_score = (circularity * 0.3 + solidity * 0.4 +
                               min(aspect_ratio, 1/aspect_ratio) * 0.3 if aspect_ratio > 0 else 0)

                valid_contours.append({
                    'contour': contour,
                    'area': area,
                    'quality': quality_score,
                    'circularity': circularity,
                    'solidity': solidity,
                    'aspect_ratio': aspect_ratio
                })

        # æŒ‰è´¨é‡å’Œé¢ç§¯æ’åº
        valid_contours.sort(key=lambda x: (x['quality'] * 0.6 + (x['area'] / (width * height)) * 0.4), reverse=True)

        segmented_objects = []

        for i, contour_info in enumerate(valid_contours[:3]):  # æœ€å¤šå–3ä¸ªæœ€å¥½çš„è½®å»“
            contour = contour_info['contour']

            # åˆ›å»ºç²¾ç¡®çš„è½®å»“æ©ç 
            mask = np.zeros(gray.shape, np.uint8)
            cv2.fillPoly(mask, [contour], 255)

            # è½®å»“å¹³æ»‘å¤„ç†
            epsilon = 0.02 * cv2.arcLength(contour, True)
            smoothed_contour = cv2.approxPolyDP(contour, epsilon, True)

            # åˆ›å»ºå¹³æ»‘åçš„æ©ç 
            smooth_mask = np.zeros(gray.shape, np.uint8)
            cv2.fillPoly(smooth_mask, [smoothed_contour], 255)

            # åº”ç”¨æ©ç åˆ°åŸå›¾åƒ
            result = image.copy()

            # åˆ›å»ºæ¸å˜è¾¹ç¼˜æ•ˆæœ
            # å¯¹æ©ç è¿›è¡Œè½»å¾®çš„é«˜æ–¯æ¨¡ç³Šï¼Œåˆ›å»ºæŸ”å’Œè¾¹ç¼˜
            blurred_mask = cv2.GaussianBlur(smooth_mask, (3, 3), 0)
            blurred_mask_3ch = cv2.cvtColor(blurred_mask, cv2.COLOR_GRAY2BGR) / 255.0

            # åº”ç”¨æ©ç 
            result = result * blurred_mask_3ch

            # å°†èƒŒæ™¯è®¾ä¸ºç™½è‰²
            background = np.ones_like(result) * 255
            result = result + background * (1 - blurred_mask_3ch)
            result = result.astype(np.uint8)

            # è·å–ç²¾ç¡®çš„è¾¹ç•Œæ¡†
            x, y, w, h = cv2.boundingRect(smoothed_contour)

            # æ™ºèƒ½è£å‰ªï¼šä¿æŒå¯¹è±¡å®Œæ•´æ€§
            # è®¡ç®—å¯¹è±¡çš„å®é™…è¾¹ç•Œ
            mask_coords = np.where(smooth_mask > 0)
            if len(mask_coords[0]) > 0:
                y_min, y_max = np.min(mask_coords[0]), np.max(mask_coords[0])
                x_min, x_max = np.min(mask_coords[1]), np.max(mask_coords[1])

                # æ·»åŠ é€‚å½“çš„è¾¹è·
                padding_x = max(10, int((x_max - x_min) * 0.05))
                padding_y = max(10, int((y_max - y_min) * 0.05))

                x1 = max(0, x_min - padding_x)
                y1 = max(0, y_min - padding_y)
                x2 = min(width, x_max + padding_x)
                y2 = min(height, y_max + padding_y)
            else:
                # å›é€€åˆ°è¾¹ç•Œæ¡†
                padding = 10
                x1 = max(0, x - padding)
                y1 = max(0, y - padding)
                x2 = min(width, x + w + padding)
                y2 = min(height, y + h + padding)

            cropped_result = result[y1:y2, x1:x2]

            # ä¿å­˜åˆ†å‰²ç»“æœ
            import time
            timestamp = int(time.time() * 1000)  # ä½¿ç”¨æ¯«ç§’çº§æ—¶é—´æˆ³é¿å…é‡å¤
            safe_object_name = object_name.replace('/', '_').replace('\\', '_').replace(' ', '_')
            seg_filename = f"opencv_contour_{safe_object_name}_{i}_{timestamp}.png"
            seg_filepath = os.path.join(current_app.config['GENERATED_FOLDER'], seg_filename)
            cv2.imwrite(seg_filepath, cropped_result)

            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = min(0.95, max(0.4, contour_info['quality']))

            # è½¬æ¢ä¸ºå½’ä¸€åŒ–åæ ‡
            ymin = y1 / height
            xmin = x1 / width
            ymax = y2 / height
            xmax = x2 / width

            segmented_objects.append({
                'label': f'{object_name}_ç²¾ç¡®è½®å»“_{i+1}',
                'description': f'åŸºäºç²¾ç¡®è½®å»“åˆ†å‰²çš„{object_name}å¯¹è±¡ï¼ˆè´¨é‡è¯„åˆ†: {contour_info["quality"]:.2f}ï¼‰',
                'confidence': confidence,
                'bbox': [ymin, xmin, ymax, xmax],
                'segment_image': seg_filepath,
                'method': 'Precise Contour Mask',
                'quality_metrics': {
                    'circularity': contour_info['circularity'],
                    'solidity': contour_info['solidity'],
                    'aspect_ratio': contour_info['aspect_ratio'],
                    'area_ratio': contour_info['area'] / (width * height)
                }
            })

        return segmented_objects

    def _grabcut_segmentation(self, image, filepath):
        """GrabCut åˆ†å‰²ç®—æ³•"""
        height, width = image.shape[:2]

        # åˆ›å»ºä¸€ä¸ªçŸ©å½¢ä½œä¸ºå‰æ™¯åŒºåŸŸï¼ˆå›¾åƒä¸­å¿ƒåŒºåŸŸï¼‰
        rect = (width//4, height//4, width//2, height//2)

        # åˆå§‹åŒ–æ©ç 
        mask = np.zeros((height, width), np.uint8)
        bgd_model = np.zeros((1, 65), np.float64)
        fgd_model = np.zeros((1, 65), np.float64)

        # åº”ç”¨ GrabCut
        cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)

        # åˆ›å»ºæœ€ç»ˆæ©ç 
        mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')
        result = image * mask2[:, :, np.newaxis]

        # ä¿å­˜åˆ†å‰²ç»“æœ
        import time
        timestamp = int(time.time() * 1000)
        seg_filename = f"opencv_grabcut_{timestamp}.png"
        seg_filepath = os.path.join(current_app.config['GENERATED_FOLDER'], seg_filename)
        cv2.imwrite(seg_filepath, result)

        return [{
            'label': 'GrabCut å‰æ™¯',
            'description': 'ä½¿ç”¨ GrabCut ç®—æ³•åˆ†å‰²çš„å‰æ™¯å¯¹è±¡',
            'confidence': 0.8,
            'bbox': [0.25, 0.25, 0.75, 0.75],  # è¿‘ä¼¼è¾¹ç•Œæ¡†
            'segment_image': seg_filepath,
            'method': 'GrabCut'
        }]

    def _watershed_segmentation(self, image, filepath):
        """Watershed åˆ†å‰²ç®—æ³•"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # åº”ç”¨é˜ˆå€¼
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

        # å™ªå£°å»é™¤
        kernel = np.ones((3, 3), np.uint8)
        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)

        # ç¡®å®šèƒŒæ™¯åŒºåŸŸ
        sure_bg = cv2.dilate(opening, kernel, iterations=3)

        # æŸ¥æ‰¾å‰æ™¯åŒºåŸŸ
        dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)
        _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)

        # æŸ¥æ‰¾æœªçŸ¥åŒºåŸŸ
        sure_fg = np.uint8(sure_fg)
        unknown = cv2.subtract(sure_bg, sure_fg)

        # æ ‡è®°æ ‡ç­¾
        _, markers = cv2.connectedComponents(sure_fg)
        markers = markers + 1
        markers[unknown == 255] = 0

        # åº”ç”¨ watershed
        markers = cv2.watershed(image, markers)
        image[markers == -1] = [255, 0, 0]

        # ä¿å­˜ç»“æœ
        import time
        timestamp = int(time.time() * 1000)
        seg_filename = f"opencv_watershed_{timestamp}.png"
        seg_filepath = os.path.join(current_app.config['GENERATED_FOLDER'], seg_filename)
        cv2.imwrite(seg_filepath, image)

        return [{
            'label': 'Watershed åˆ†å‰²',
            'description': 'ä½¿ç”¨ Watershed ç®—æ³•åˆ†å‰²çš„åŒºåŸŸ',
            'confidence': 0.7,
            'bbox': [0.1, 0.1, 0.9, 0.9],
            'segment_image': seg_filepath,
            'method': 'Watershed'
        }]

    def _kmeans_segmentation(self, image, filepath):
        """K-means èšç±»åˆ†å‰²"""
        # é‡å¡‘å›¾åƒæ•°æ®
        data = image.reshape((-1, 3))
        data = np.float32(data)

        # å®šä¹‰æ ‡å‡†å¹¶åº”ç”¨ K-means
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        k = 3  # èšç±»æ•°é‡
        _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

        # è½¬æ¢å› uint8 å¹¶é‡å¡‘ä¸ºåŸå§‹å›¾åƒå½¢çŠ¶
        centers = np.uint8(centers)
        segmented_data = centers[labels.flatten()]
        segmented_image = segmented_data.reshape(image.shape)

        # ä¿å­˜ç»“æœ
        import time
        timestamp = int(time.time() * 1000)
        seg_filename = f"opencv_kmeans_{timestamp}.png"
        seg_filepath = os.path.join(current_app.config['GENERATED_FOLDER'], seg_filename)
        cv2.imwrite(seg_filepath, segmented_image)

        return [{
            'label': 'K-means èšç±»',
            'description': f'ä½¿ç”¨ K-means ç®—æ³•åˆ†å‰²ä¸º {k} ä¸ªåŒºåŸŸ',
            'confidence': 0.6,
            'bbox': [0.0, 0.0, 1.0, 1.0],
            'segment_image': seg_filepath,
            'method': 'K-means'
        }]


